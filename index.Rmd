---
title: "Predicting the Quality of a Workout"
author: "Ben Kling"
date: "4/15/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(parallel)
library(doParallel)
library(randomForest)
```

## Predicting Quality of a workout

In this project, we will attempt to use statistical learning to make predictions
about the manner in which a user performed an exercise. We are given data from 
accelerometers that take measurements from the belt, forearm, arm, and dumbbell.

The goal is to accurately predict a value called "classe" given a sample of
19,622 labelled observations.


## Data input and Cleaning
The data set requires us to do some cleaning to ensure it is ready to be 
modeled properly. Brief outline of cleaning steps taken:

* Coerced all features to numeric - there were many NA values and many values
that appear to be artifacts of storage in excel ("#DIV/0" values)
* replaced NA values with 0 - models need to interpret all values as continuous
and without replacing to zero these observations would be ignored.
* removed features that have no non-zero values: 9 potential features are 
completely absent of any observed data. These need to be removed prior to the 
execution of the data to prevent complications during modeling

In addition to cleaning the data, we will also partition the data known as the 
"training" data into two sets:

* 80% to train the model

* 20% to test the model and estimate out of sample error, prior to applying the
model against the 20 evaluation observations

```{r file download and data frame initialization, echo=FALSE, message=FALSE, warning=FALSE}
# Download the training file
if(!file.exists("pml-training.csv")) download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","pml-training.csv")
# Download the test file
if(!file.exists("pml-testing.csv")) download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","pml-testing.csv")

# establish data frame
rawdata <- read_csv("pml-training.csv")
evaluationraw <- read_csv("pml-testing.csv")

```

```{r data cleaning, message=FALSE}

# #DIV/0! values
data <- rawdata %>%
    mutate(across(.fns=na_if, y="#DIV/0!")) %>% # lots of "#DIV/0" errors in the data (excel output?)
    mutate(across(.cols = -c(1:7,160), .fns = as.numeric)) %>% # Coerce to numeric
    mutate(across(.cols = -c(1:7,160), .fns = replace_na, replace=0)) %>% # zero out the NA values
    filter(!is.na(classe)) %>% # remove NA classe values
    mutate(classe = as.factor(classe)) # convert classe to a factor


evaluationdata <- evaluationraw %>%
    mutate(across(.fns=na_if, y="#DIV/0!")) %>% # lots of "#DIV/0" errors in the data (excel output?)
    mutate(across(.cols = -c(1:7,160), .fns = as.numeric)) %>% # Coerce to numeric
    mutate(across(.cols = -c(1:7,160), .fns = replace_na, replace=0)) # zero out the NA values




# these features have ONLY ZERO values
allzero <- c('kurtosis_yaw_belt',
             'skewness_yaw_belt', 
             'amplitude_yaw_belt', 
             'kurtosis_yaw_dumbbell', 
             'skewness_yaw_dumbbell', 
             'amplitude_yaw_dumbbell', 
             'kurtosis_yaw_forearm', 
             'skewness_yaw_forearm', 
             'amplitude_yaw_forearm')

data <- data %>%
    select(-allzero)

evaluationdata <- evaluationdata %>%
    select(-allzero)

# first 7 variables are not measurements, merely attributes and can be ignored
data_processed <- data %>% select(-(1:7)) 
evaluation_processed <- evaluationdata %>% select(-(1:7))


# hold out 20 % of training data for validation purposes, to estimate out of 
# sample error

# set seed for reproducibility
set.seed(1234)

trainIndex <- createDataPartition(data_processed$classe,
                                  p=0.8,
                                  list=FALSE,
                                  times=1
                                  )

train_data <- data_processed[trainIndex,]
test_data <- data_processed[-trainIndex,]

```


## Analysis

### Exploratory Data Analysis

Prior to selecting a model, we run some exploratory data analysis to determine 
what the structure of the data is that we are looking at as well as visualizing 
distribution of the features across the labelled data. I've also included some
code from QA against the data to ensure it was cleaned prioperly.

```{r eda plots, message=FALSE, warning=FALSE}

g <- ggplot(train_data, mapping = aes(max_roll_belt, fill=classe, color=classe))
g + 
    geom_histogram() + 
    facet_wrap(~classe) + 
    ggtitle("Histogram of an example feature and its distribution of values for
            each classe")


g <- ggplot(train_data, mapping = aes(classe, fill=classe))
g + 
    geom_histogram(stat="count") + 
    ggtitle("Number of Observations for Each Classe (training slice)")
```

```{r eda and qa slicing, include=FALSE}

for (var in allzero) {
    rawdata %>%
        group_by(.data[[var]]) %>% 
        summarize(ct = n()) %>%
        head(n=12) %>%
        print()
}

```


### Model Selection
Given that this is a classification problem, I have chosen to use a random 
forest model with which to build a prediction. There are many possible models
to choose from, but with how robust of a sample that we are given it is likely
that many models will perform well.


```{r model selection, cache=TRUE}

# apply pca
# run random forest on the reduced feature set
# parallellize the execution

set.seed(1234) #set a seed for reproducibility

cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitCtrl <- trainControl(method='cv',
                        number=5,
                        allowParallel=TRUE)

rfmodel <- train(classe ~.,
                 data=train_data,
                 method='rf',
                 trControl=fitCtrl)

stopCluster(cluster)
registerDoSEQ()
```

### Model Parameters & Performance

In addition to selecting random forest as our model of choice, we also are 
applying k-fold cross-validation to the result set, with 5 folds.

After running the model using the "caret" package, we see that the optimal value
for the only tuning parameter (mtry) is 72, which produces a 99.3% accuracy 
on the training set

*Accuracy - in sample, out of sample*
Upon running the model, we see the following for accuracy:

**In sample (against the training set):** 99.3% (95% CI: 90.0% - 99.5%)
```{r model evaluation}

rfmodel 

rfmodel$resample

confusionMatrix.train(rfmodel)

```


We held out a group of the training data to estimate our out of sample accuracy
as well, given how robust of a dataset we had. 

**Out of sample Accuracy:** 99.3%


**Confusion Matrix:**
```{r predicting test data}

pred <- predict(rfmodel, 
                test_data)

confusionMatrix(test_data$classe,pred)

```


**Predictions against the evaluation (unlabelled) set**

Here we run some code to see what the model chose for predicting each of the 20
unlabelled observations:

```{r predict the evaluation data1}

eval_pred <- predict(rfmodel,
                     evaluation_processed)

# print predictions
i <- 1
for (j in eval_pred) {
    print(paste0(i," ", j))    
    i <- i + 1
}


```


*Performance Notes*

Running the random forest modeling initially was very slow. In order to produce
results quicker, we replaced bootstrap resampling (very laborious) with 
cross-validation. In addition we used the doParallel package to allow the model
to be trained with parallel computing. This sped up the runtime from 
indefinitely long, to just a minute or two.












### Citations and References

All data in this analysis was made available here: <https://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har>
An excellent guide to parallelizing model training by Leonard Greski, here <https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md>
